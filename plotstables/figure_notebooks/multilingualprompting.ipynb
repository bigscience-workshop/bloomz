{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZnW0d-en1Di",
        "outputId": "85a015d1-05f6-4bbd-afb5-0c82d0533a76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets==2.5.0\n",
            "  Downloading datasets-2.5.0-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.2/431.2 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.5.0) (4.65.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.5.0) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.5.0) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets==2.5.0) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets==2.5.0) (23.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.5.0) (2023.3.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.5.0) (1.4.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.5.0) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.5.0) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.5.0) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.5.0) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.5.0) (6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.5.0) (3.4)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.5.0) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.5.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.5.0) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.5.0 dill-0.3.5.1 frozenlist-1.3.3 huggingface-hub-0.13.4 multidict-6.0.4 multiprocess-0.70.13 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n",
            "Cloning into 'evaluation-results'...\n",
            "remote: Enumerating objects: 90411, done.\u001b[K\n",
            "remote: Counting objects: 100% (305/305), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 90411 (delta 127), reused 298 (delta 122), pack-reused 90106\u001b[K\n",
            "Receiving objects: 100% (90411/90411), 38.57 MiB | 16.31 MiB/s, done.\n",
            "Resolving deltas: 100% (42065/42065), done.\n",
            "Updating files: 100% (47486/47486), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets==2.5.0 # https://github.com/huggingface/datasets/issues/5111\n",
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/bigscience/evaluation-results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.utils.logging import set_verbosity_error\n",
        "set_verbosity_error()\n",
        "\n",
        "from datasets import disable_progress_bar\n",
        "disable_progress_bar()"
      ],
      "metadata": {
        "id": "qBElHu1zn11X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "bloom = load_dataset(\"evaluation-results\", \"bloom\")\n",
        "bloom_7b1 = load_dataset(\"evaluation-results\", \"bloom-7b1\")\n",
        "bloom_3b = load_dataset(\"evaluation-results\", \"bloom-3b\")\n",
        "bloom_1b7 = load_dataset(\"evaluation-results\", \"bloom-1b7\")\n",
        "bloom_1b1 = load_dataset(\"evaluation-results\", \"bloom-1b1\")\n",
        "bloom_560m = load_dataset(\"evaluation-results\", \"bloom-560m\")\n",
        "\n",
        "\n",
        "bloomz = load_dataset(\"evaluation-results\", \"bloomz\")\n",
        "bloomz_7b1 = load_dataset(\"evaluation-results\", \"bloomz-7b1\")\n",
        "bloomz_3b = load_dataset(\"evaluation-results\", \"bloomz-3b\")\n",
        "bloomz_1b7 = load_dataset(\"evaluation-results\", \"bloomz-1b7\")\n",
        "bloomz_1b1 = load_dataset(\"evaluation-results\", \"bloomz-1b1\")\n",
        "bloomz_560m = load_dataset(\"evaluation-results\", \"bloomz-560m\")\n",
        "\n",
        "\n",
        "bloomz_mt = load_dataset(\"evaluation-results\", \"bloomz-mt\")\n",
        "bloomz_7b1_mt = load_dataset(\"evaluation-results\", \"bloomz-7b1-mt\")\n",
        "\n",
        "bloomz_7b1_p3 = load_dataset(\"evaluation-results\", \"bloomz-7b1-p3\")\n",
        "bloomz_p3 = load_dataset(\"evaluation-results\", \"bloomz-p3\")\n",
        "\n",
        "mt0_xxl = load_dataset(\"evaluation-results\", \"mt0-xxl\")\n",
        "mt0_xxl_mt = load_dataset(\"evaluation-results\", \"mt0-xxl-mt\")\n",
        "\n",
        "mt5_xxl = load_dataset(\"evaluation-results\", \"mt5-xxl\")"
      ],
      "metadata": {
        "id": "XzXaQhMNn3op",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b988db2-54a8-4780-8380-aa70c5701434"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset evaluation-results/bloom to /root/.cache/huggingface/datasets/evaluation-results/bloom/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloom/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloom-7b1 to /root/.cache/huggingface/datasets/evaluation-results/bloom-7b1/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloom-7b1/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloom-3b to /root/.cache/huggingface/datasets/evaluation-results/bloom-3b/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloom-3b/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloom-1b7 to /root/.cache/huggingface/datasets/evaluation-results/bloom-1b7/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloom-1b7/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloom-1b1 to /root/.cache/huggingface/datasets/evaluation-results/bloom-1b1/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloom-1b1/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloom-560m to /root/.cache/huggingface/datasets/evaluation-results/bloom-560m/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloom-560m/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz to /root/.cache/huggingface/datasets/evaluation-results/bloomz/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-7b1 to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-3b to /root/.cache/huggingface/datasets/evaluation-results/bloomz-3b/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-3b/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-1b7 to /root/.cache/huggingface/datasets/evaluation-results/bloomz-1b7/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-1b7/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-1b1 to /root/.cache/huggingface/datasets/evaluation-results/bloomz-1b1/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-1b1/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-560m to /root/.cache/huggingface/datasets/evaluation-results/bloomz-560m/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-560m/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-mt to /root/.cache/huggingface/datasets/evaluation-results/bloomz-mt/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-mt/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-7b1-mt to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1-mt/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1-mt/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-7b1-p3 to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1-p3/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1-p3/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-p3 to /root/.cache/huggingface/datasets/evaluation-results/bloomz-p3/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-p3/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/mt0-xxl to /root/.cache/huggingface/datasets/evaluation-results/mt0-xxl/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/mt0-xxl/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/mt0-xxl-mt to /root/.cache/huggingface/datasets/evaluation-results/mt0-xxl-mt/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/mt0-xxl-mt/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/mt5-xxl to /root/.cache/huggingface/datasets/evaluation-results/mt5-xxl/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/mt5-xxl/1.0.0/067a2865db6be00ca82d02e9f53c11ba6da5da780c34a99cb5eba63ceacb77b8. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "EVAL_MODELS = {\n",
        "    \"BLOOMZ\": bloomz,\n",
        "    \"BLOOMZ-MT\": bloomz_mt,\n",
        "    \"mT0\": mt0_xxl,\n",
        "    \"mT0-MT\": mt0_xxl_mt,  \n",
        "}\n",
        "\n",
        "EVAL_TASKS = {\n",
        "    \"xnli\": \"XNLI\",\n",
        "    \"xcopa\": \"XCOPA\",\n",
        "    \"Muennighoff/xstory_cloze\": \"XStoryCloze\",\n",
        "    \"Muennighoff/xwinograd\": \"XWinograd\",\n",
        "}\n",
        "\n",
        "EVAL_LANGS = ['en', 'es', 'pt', 'fr', 'ar', 'id', 'zh', 'hi', 'vi', 'ur', 'ta', 'eu', 'te', 'sw']\n",
        "\n",
        "for task_name in EVAL_TASKS:\n",
        "    score_en = {}\n",
        "    score_mt = {}\n",
        "    score_ht = {}\n",
        "    for name in EVAL_MODELS:\n",
        "        model_type = EVAL_MODELS[name]\n",
        "        \n",
        "        task_ds = model_type['test'].filter(lambda x: x[\"task_name\"].startswith(task_name))\n",
        "      \n",
        "        prompt_ds_ht = task_ds.filter(lambda x: x[\"prompt_name\"].endswith(\"ht\"))\n",
        "        prompt_ds_mt = task_ds.filter(lambda x: x[\"prompt_name\"].endswith(\"mt\"))\n",
        "        prompt_ds_en = task_ds.filter(lambda x: not(x[\"prompt_name\"].endswith((\"ht\", \"mt\"))))\n",
        "        \n",
        "        score_en[name] = np.mean([x[\"score\"] for x in prompt_ds_en if x['task_name'][-2:] in EVAL_LANGS])\n",
        "        score_mt[name] = np.mean([x[\"score\"] for x in prompt_ds_mt if x['task_name'][-2:] in EVAL_LANGS])\n",
        "        score_ht[name] = np.mean([x[\"score\"] for x in prompt_ds_ht if x['task_name'][-2:] in EVAL_LANGS]) if len(prompt_ds_ht) else -1\n",
        "\n",
        "    def print_line(scores):\n",
        "        # max scores for BLOOMZ and mT0\n",
        "        best_scores = [np.max([scores[name] for name in ['BLOOMZ', 'BLOOMZ-MT']]), \n",
        "                       np.max([scores[name] for name in ['mT0', 'mT0-MT']])]\n",
        "        for name in EVAL_MODELS:\n",
        "            if scores[name] in best_scores:\n",
        "              print(\" & \\\\textbf{\" + str(round(scores[name] * 100,2)) + \"}\", end = '')\n",
        "            else:\n",
        "              print(\" & \" + str(round(scores[name] * 100,2)), end = '')\n",
        "        print(\" \\\\\\\\\")\n",
        "\n",
        "    print(\"\\\\midrule\")\n",
        "    print(EVAL_TASKS[task_name], end='')\n",
        "    print(\" & EN\", end='')\n",
        "    print_line(score_en)\n",
        "    \n",
        "    print(\" & MT\", end='')\n",
        "    print_line(score_mt)\n",
        "\n",
        "    if task_name == \"xnli\":\n",
        "      print(\" & HT\", end='')\n",
        "      print_line(score_ht)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MkKyxfqcFsd",
        "outputId": "6b3a100b-baa3-4a4a-dbd3-fe9f1c67d3c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\midrule\n",
            "XNLI & EN & \\textbf{52.99} & 49.01 & 48.24 & \\textbf{51.29} \\\\\n",
            " & MT & 37.56 & \\textbf{41.16} & 39.31 & \\textbf{41.66} \\\\\n",
            " & HT & 40.4 & \\textbf{43.88} & 44.95 & \\textbf{46.87} \\\\\n",
            "\\midrule\n",
            "XCOPA & EN & 72.52 & \\textbf{73.24} & \\textbf{81.4} & 80.36 \\\\\n",
            " & MT & 70.04 & \\textbf{71.84} & \\textbf{81.16} & 79.64 \\\\\n",
            "\\midrule\n",
            "XStoryCloze & EN & \\textbf{81.73} & 81.39 & 81.99 & \\textbf{82.3} \\\\\n",
            " & MT & 80.89 & \\textbf{81.76} & \\textbf{83.37} & 82.86 \\\\\n",
            "\\midrule\n",
            "XWinograd & EN & \\textbf{60.07} & 59.15 & 70.49 & \\textbf{73.24} \\\\\n",
            " & MT & 58.48 & \\textbf{60.14} & 66.89 & \\textbf{72.33} \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired by Appendix H from GPT-3\n",
        "# Columns: Model ()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Languages not pretrained on\n",
        "l2_xwino = [\"jp\", \"ru\"]\n",
        "l2_xstory = [\"ru\", \"my\"]\n",
        "l2_xcopa = [\"et\", \"ht\", \"it\", \"qu\", \"tr\"]\n",
        "l2_xnli = [\"bg\", \"de\", \"el\", \"ru\", \"th\", \"tr\"]\n",
        "L2 = set(l2_xwino + l2_xstory + l2_xcopa + l2_xnli)\n",
        "\n",
        "# Langs ordered by PCTG in xP3\n",
        "LANGS = ['en', 'es', 'pt', 'fr', 'ar', 'id', 'zh', 'hi', 'code', 'vi', 'ur', 'te', 'ta', 'bn', 'mr', 'sw', 'gu', 'pa', 'ne', 'yo', 'ig', 'ny', 'zu', 'xh', 'sn', 'ts', 'rw', 'lg', 'tn', 'nso', 'rn', 'ml', 'kn', 'or', 'as', 'ln', 'wo', 'tum', 'ki', 'st', 'fon', 'ca', 'eu', 'ak', 'bm', 'tw']\n",
        "EVAL_LANGS = ['en', 'es', 'pt', 'fr', 'ar', 'id', 'zh', 'hi', 'vi', 'ur', 'ta', 'eu', 'te', 'sw']\n",
        "EVAL_LANGS += [\"Avg\"] # Add Average score at the end\n",
        "\n",
        "DS_TO_NAME = {\n",
        "    \"anli\": \"ANLI\",\n",
        "    \"xnli\": \"XNLI\",\n",
        "    \"super_glue\": \"SuperGLUE\",\n",
        "    \"xcopa\": \"XCOPA\",\n",
        "    \"story_cloze\": \"StoryCloze\",\n",
        "    \"Muennighoff/xstory_cloze\": \"XStoryCloze\",\n",
        "    \"winogrande\": \"Winogrande XL\",\n",
        "    \"Muennighoff/xwinograd\": \"XWinograd\",\n",
        "}\n",
        "\n",
        "DS_TO_SPLIT = {\n",
        "    \"super_glue\": \"validation\",\n",
        "    \"xnli\": \"validation\",\n",
        "    \"story_cloze\": \"validation\",\n",
        "    \"Muennighoff/xstory_cloze\": \"validation\",\n",
        "    \"xcopa\": \"validation\",\n",
        "    \"winogrande\": \"validation\",\n",
        "    \"Muennighoff/xwinograd\": \"test\"\n",
        "}\n",
        "\n",
        "MT5_MODEL_TO_RES = {\n",
        "    \"mT5 XXL\": mt5_xxl\n",
        "}\n",
        "\n",
        "MT0_MODEL_TO_RES = {\n",
        "    \"mT0-13B\": mt0_xxl\n",
        "}\n",
        "\n",
        "BLOOM_MODEL_TO_RES = {\n",
        "    \"BLOOM-560M\": bloom_560m,\n",
        "    \"BLOOM-1.1B\": bloom_1b1,\n",
        "    \"BLOOM-1.7B\": bloom_1b7,\n",
        "    \"BLOOM-3B\": bloom_3b,\n",
        "    \"BLOOM-7.1B\": bloom_7b1,\n",
        "    \"BLOOM\": bloom,\n",
        "}\n",
        "\n",
        "BLOOMZ_MODEL_TO_RES = {\n",
        "    \"BLOOMZ-560M\": bloomz_560m,\n",
        "    \"BLOOMZ-1.1B\": bloomz_1b1,\n",
        "    \"BLOOMZ-1.7B\": bloomz_1b7,\n",
        "    \"BLOOMZ-3B\": bloomz_3b,\n",
        "    \"BLOOMZ-7.1B\": bloomz_7b1,\n",
        "    \"BLOOMZ-7.1B-MT\": bloomz_7b1_mt,\n",
        "    \"BLOOMZ-7.1B-P3\": bloomz_7b1_p3,\n",
        "    \"BLOOMZ\": bloomz,\n",
        "    \"BLOOMZ-MT\": bloomz_mt,\n",
        "    \"BLOOMZ-P3\": bloomz_p3,    \n",
        "}\n",
        "\n",
        "def get_task(task_name):\n",
        "    if \"wino\" in task_name: return \"Coref. Res.\"\n",
        "    elif (\"story\" in task_name) or (\"copa\" in task_name): return \"Completion\"\n",
        "    return \"NLI\"\n",
        "\n",
        "HEADER = \"\\multicolumn{6}{c}{} & \\multicolumn{6}{c}{Pretrained} & \\multicolumn{11}{c}{Pretrained + Multitask finetuned} \\\\\\\\\"\n",
        "HEADER += \"\\n\" + \"Task & Dataset & Config & Split & Prompt & Metric\"\n",
        "HEADER += \" & \" + \" & \".join(list(BLOOM_MODEL_TO_RES.keys()))\n",
        "HEADER += \" & \" + \" & \".join(list(BLOOMZ_MODEL_TO_RES.keys()))\n",
        "HEADER += \" & \" + \" & \".join(list(MT0_MODEL_TO_RES.keys()))\n",
        "HEADER += \" \\\\\\\\\"\n",
        "\n",
        "TABLE = HEADER\n",
        "\n",
        "RES_DICT = {}\n",
        "for ds, ds_name in DS_TO_NAME.items():\n",
        "    print(f\"Running dataset {ds}\")\n",
        "    for name, res_data in {**BLOOM_MODEL_TO_RES, **BLOOMZ_MODEL_TO_RES, **MT0_MODEL_TO_RES}.items():\n",
        "        ds_data = res_data[\"test\"].filter(lambda x: (x[\"evaluation_framework\"]  == \"bigscience/bloomz\") and (x[\"task_name\"].startswith(ds)), load_from_cache_file=False)\n",
        "\n",
        "        # Iterate through subdatasets\n",
        "        for task_name in set(ds_data[\"task_name\"]):\n",
        "            task_ds = ds_data.filter(lambda x: x[\"task_name\"] == task_name)\n",
        "            config = task_name.split(\"_\")[-1]\n",
        "            task = get_task(task_name)\n",
        "            split = DS_TO_SPLIT.get(ds, \"validation\")\n",
        "\n",
        "            #print(res_data)\n",
        "            #print(f\"Unexpected len {set(task_ds['prompt_name'])} for {ds} for {task_name} for {name}.\")\n",
        "\n",
        "            prompt_ds_ht = task_ds.filter(lambda x: x[\"prompt_name\"].endswith(\"ht\"))\n",
        "            prompt_ds_mt = task_ds.filter(lambda x: x[\"prompt_name\"].endswith(\"mt\"))\n",
        "            prompt_ds_en = task_ds.filter(lambda x: not(x[\"prompt_name\"].endswith((\"ht\", \"mt\"))))\n",
        "\n",
        "            for prompt_ds, prompt in [(prompt_ds_en, \"EN\"), (prompt_ds_ht, \"HT\"), (prompt_ds_mt, \"MT\")]:\n",
        "                if len(prompt_ds) == 0: continue\n",
        "                elif len(prompt_ds) != 5: print(f\"Unexpected len {len(prompt_ds)} for {ds} for {task_name} for {prompt} for {name}.\")\n",
        "                score_median = np.median([x[\"score\"] for x in prompt_ds])\n",
        "                score_max = np.max([x[\"score\"] for x in prompt_ds])\n",
        "\n",
        "                RES_DICT.setdefault(task, {})\n",
        "                RES_DICT[task].setdefault(ds, {})\n",
        "                RES_DICT[task][ds].setdefault(config, {})\n",
        "                RES_DICT[task][ds][config].setdefault(split, {})\n",
        "                RES_DICT[task][ds][config][split].setdefault(prompt, {})\n",
        "                RES_DICT[task][ds][config][split][prompt].setdefault(\"Median Acc\", {})\n",
        "                RES_DICT[task][ds][config][split][prompt].setdefault(\"Max Acc\", {})\n",
        "                RES_DICT[task][ds][config][split][prompt][\"Median Acc\"][name] = format(score_median * 100, '.2f')\n",
        "                RES_DICT[task][ds][config][split][prompt][\"Max Acc\"][name] = format(score_max * 100, '.2f')\n",
        "\n",
        "\n",
        "task, ds, config, split, prompt = \"Program Synthesis\", \"openai_humaneval\", \"None\", \"test\", \"EN\"\n",
        "for name, res_data in {**BLOOM_MODEL_TO_RES, **BLOOMZ_MODEL_TO_RES, **MT0_MODEL_TO_RES}.items():\n",
        "    ds_data = res_data[\"test\"].filter(lambda x: (x[\"evaluation_framework\"]  == \"bloom-code-evaluation\"), load_from_cache_file=False)\n",
        "    for k in [1, 10, 100]:\n",
        "        k_data = ds_data.filter(lambda x: x[\"metric\"].startswith(f\"pass@{k}-\"))\n",
        "        if len(k_data) == 0: continue\n",
        "        RES_DICT.setdefault(task, {})\n",
        "        RES_DICT[task].setdefault(ds, {})\n",
        "        RES_DICT[task][ds].setdefault(config, {})\n",
        "        RES_DICT[task][ds][config].setdefault(split, {})\n",
        "        RES_DICT[task][ds][config][split].setdefault(prompt, {})\n",
        "        RES_DICT[task][ds][config][split][prompt].setdefault(f\"Pass@{k}\", {})\n",
        "        RES_DICT[task][ds][config][split][prompt][f\"Pass@{k}\"][name] = format(np.max(k_data[\"score\"]) * 100, '.2f')\n",
        "\n",
        "\n",
        "for task, vals in RES_DICT.items():\n",
        "    for ds, sub_vals in sorted(vals.items()):\n",
        "        for config, sub_sub_vals in sorted(sub_vals.items()):\n",
        "            for split, sub_sub_sub_vals in sorted(sub_sub_vals.items()):\n",
        "                for prompt, sub_sub_sub_sub_vals in sorted(sub_sub_sub_vals.items()):\n",
        "                    for metric, sub_sub_sub_sub_sub_vals in sorted(sub_sub_sub_sub_vals.items()):\n",
        "                        ONE_LINE = f\"{task} & {ds} & {config} & {split} & {prompt} & {metric}\"\n",
        "                        #for name in MT5_MODEL_TO_RES:\n",
        "                        #    ONE_LINE += \" & \" + sub_sub_sub_sub_sub_vals.get(name, \"-\")\n",
        "                        for name in BLOOM_MODEL_TO_RES:\n",
        "                            ONE_LINE += \" & \" + sub_sub_sub_sub_sub_vals.get(name, \"-\")\n",
        "                        for name in BLOOMZ_MODEL_TO_RES:\n",
        "                            ONE_LINE += \" & \" + sub_sub_sub_sub_sub_vals.get(name, \"-\")\n",
        "                        for name in MT0_MODEL_TO_RES:\n",
        "                            ONE_LINE += \" & \" + sub_sub_sub_sub_sub_vals.get(name, \"-\")                            \n",
        "                        TABLE += \"\\n\" + ONE_LINE + \"\\\\\\\\\"\n",
        "\n",
        "# Escape _ in Latex\n",
        "TABLE = TABLE.replace(\"_\", \"\\_\")\n",
        "TABLE = TABLE.replace(\"Muennighoff/\", \"\")\n"
      ],
      "metadata": {
        "id": "pkx4dnV1ocR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TABLE)"
      ],
      "metadata": {
        "id": "QoJL3nyDNcaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z6u8IqWcu4bd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}