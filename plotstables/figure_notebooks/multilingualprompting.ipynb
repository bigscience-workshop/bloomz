{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZnW0d-en1Di",
        "outputId": "ebf0a76d-1020-43d4-de12-7777133215c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets==2.5.0\n",
            "  Downloading datasets-2.5.0-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==2.5.0) (1.21.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets==2.5.0) (3.8.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets==2.5.0) (4.64.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==2.5.0) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==2.5.0) (21.3)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==2.5.0) (1.3.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 60.6 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==2.5.0) (2.23.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets==2.5.0) (2022.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==2.5.0) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.5.0) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.5.0) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.5.0) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.5.0) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.5.0) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.5.0) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.5.0) (4.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.5.0) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.5.0) (1.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.5.0) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.5.0) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==2.5.0) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.5.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.5.0) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 66.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==2.5.0) (3.10.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==2.5.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==2.5.0) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==2.5.0) (1.15.0)\n",
            "Installing collected packages: urllib3, dill, xxhash, responses, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "Successfully installed datasets-2.5.0 dill-0.3.5.1 huggingface-hub-0.10.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n",
            "Cloning into 'evaluation-results'...\n",
            "remote: Enumerating objects: 86319, done.\u001b[K\n",
            "remote: Counting objects: 100% (86319/86319), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16808/16808), done.\u001b[K\n",
            "remote: Total 86319 (delta 40213), reused 84201 (delta 38683), pack-reused 0\n",
            "Receiving objects: 100% (86319/86319), 37.92 MiB | 10.03 MiB/s, done.\n",
            "Resolving deltas: 100% (40213/40213), done.\n",
            "Checking out files: 100% (45896/45896), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets==2.5.0 # https://github.com/huggingface/datasets/issues/5111\n",
        "!git clone https://huggingface.co/datasets/bigscience/evaluation-results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.utils.logging import set_verbosity_error\n",
        "set_verbosity_error()\n",
        "\n",
        "from datasets import disable_progress_bar\n",
        "disable_progress_bar()"
      ],
      "metadata": {
        "id": "qBElHu1zn11X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "bloom = load_dataset(\"evaluation-results\", \"bloom\")\n",
        "bloom_7b1 = load_dataset(\"evaluation-results\", \"bloom-7b1\")\n",
        "bloom_3b = load_dataset(\"evaluation-results\", \"bloom-3b\")\n",
        "bloom_1b7 = load_dataset(\"evaluation-results\", \"bloom-1b7\")\n",
        "bloom_1b1 = load_dataset(\"evaluation-results\", \"bloom-1b1\")\n",
        "bloom_560m = load_dataset(\"evaluation-results\", \"bloom-560m\")\n",
        "\n",
        "\n",
        "bloomz = load_dataset(\"evaluation-results\", \"bloomz\")\n",
        "bloomz_7b1 = load_dataset(\"evaluation-results\", \"bloomz-7b1\")\n",
        "bloomz_3b = load_dataset(\"evaluation-results\", \"bloomz-3b\")\n",
        "bloomz_1b7 = load_dataset(\"evaluation-results\", \"bloomz-1b7\")\n",
        "bloomz_1b1 = load_dataset(\"evaluation-results\", \"bloomz-1b1\")\n",
        "bloomz_560m = load_dataset(\"evaluation-results\", \"bloomz-560m\")\n",
        "\n",
        "\n",
        "bloomz_mt = load_dataset(\"evaluation-results\", \"bloomz-mt\")\n",
        "bloomz_7b1_mt = load_dataset(\"evaluation-results\", \"bloomz-7b1-mt\")\n",
        "\n",
        "bloomz_7b1_p3 = load_dataset(\"evaluation-results\", \"bloomz-7b1-p3\")\n",
        "bloomz_p3 = load_dataset(\"evaluation-results\", \"bloomz-p3\")\n",
        "\n",
        "mt0_xxl = load_dataset(\"evaluation-results\", \"mt0-xxl\")\n",
        "mt0_xxl_mt = load_dataset(\"evaluation-results\", \"mt0-xxl-mt\")\n",
        "\n",
        "mt5_xxl = load_dataset(\"evaluation-results\", \"mt5-xxl\")"
      ],
      "metadata": {
        "id": "XzXaQhMNn3op",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc2dad3-f345-41f0-a07b-655cb9a8e475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset evaluation-results/bloom-1b1 to /root/.cache/huggingface/datasets/evaluation-results/bloom-1b1/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloom-1b1/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloom-560m to /root/.cache/huggingface/datasets/evaluation-results/bloom-560m/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloom-560m/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz to /root/.cache/huggingface/datasets/evaluation-results/bloomz/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-7b1 to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-3b to /root/.cache/huggingface/datasets/evaluation-results/bloomz-3b/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-3b/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-1b7 to /root/.cache/huggingface/datasets/evaluation-results/bloomz-1b7/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-1b7/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-1b1 to /root/.cache/huggingface/datasets/evaluation-results/bloomz-1b1/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-1b1/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-560m to /root/.cache/huggingface/datasets/evaluation-results/bloomz-560m/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-560m/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-mt to /root/.cache/huggingface/datasets/evaluation-results/bloomz-mt/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-mt/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-7b1-mt to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1-mt/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1-mt/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-7b1-p3 to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1-p3/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-7b1-p3/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/bloomz-p3 to /root/.cache/huggingface/datasets/evaluation-results/bloomz-p3/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/bloomz-p3/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/mt0-xxl to /root/.cache/huggingface/datasets/evaluation-results/mt0-xxl/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/mt0-xxl/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/mt0-xxl-mt to /root/.cache/huggingface/datasets/evaluation-results/mt0-xxl-mt/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/mt0-xxl-mt/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset evaluation-results/mt5-xxl to /root/.cache/huggingface/datasets/evaluation-results/mt5-xxl/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e...\n",
            "Dataset evaluation-results downloaded and prepared to /root/.cache/huggingface/datasets/evaluation-results/mt5-xxl/1.0.0/94bec96f1bf52030cd1d63cbd84b8620d21e2c9d9127c1bc7575ddc71b31932e. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "EVAL_MODELS = {\n",
        "    \"BLOOMZ\": bloomz,\n",
        "    \"BLOOMZ-MT\": bloomz_mt,\n",
        "    \"mT0\": mt0_xxl,\n",
        "    \"mT0-MT\": mt0_xxl_mt,  \n",
        "}\n",
        "\n",
        "EVAL_TASKS = {\n",
        "    \"xnli\": \"XNLI\",\n",
        "    \"xcopa\": \"XCOPA\",\n",
        "    \"Muennighoff/xstory_cloze\": \"XStoryCloze\",\n",
        "    \"Muennighoff/xwinograd\": \"XWinograd\",\n",
        "}\n",
        "\n",
        "EVAL_LANGS = ['en', 'es', 'pt', 'fr', 'ar', 'id', 'zh', 'hi', 'vi', 'ur', 'ta', 'eu',]\n",
        "\n",
        "for task_name in EVAL_TASKS:\n",
        "    score_en = {}\n",
        "    score_mt = {}\n",
        "    score_ht = {}\n",
        "    for name in EVAL_MODELS:\n",
        "        model_type = EVAL_MODELS[name]\n",
        "        \n",
        "        task_ds = model_type['test'].filter(lambda x: x[\"task_name\"].startswith(task_name))\n",
        "      \n",
        "        prompt_ds_ht = task_ds.filter(lambda x: x[\"prompt_name\"].endswith(\"ht\"))\n",
        "        prompt_ds_mt = task_ds.filter(lambda x: x[\"prompt_name\"].endswith(\"mt\"))\n",
        "        prompt_ds_en = task_ds.filter(lambda x: not(x[\"prompt_name\"].endswith((\"ht\", \"mt\"))))\n",
        "        \n",
        "        score_en[name] = np.mean([x[\"score\"] for x in prompt_ds_en if x['task_name'][-2:] in EVAL_LANGS])\n",
        "        score_mt[name] = np.mean([x[\"score\"] for x in prompt_ds_mt if x['task_name'][-2:] in EVAL_LANGS])\n",
        "        score_ht[name] = np.mean([x[\"score\"] for x in prompt_ds_ht if x['task_name'][-2:] in EVAL_LANGS]) if len(prompt_ds_ht) else -1\n",
        "\n",
        "    def print_line(scores):\n",
        "        # max scores for BLOOMZ and mT0\n",
        "        best_scores = [np.max([scores[name] for name in ['BLOOMZ', 'BLOOMZ-MT']]), \n",
        "                       np.max([scores[name] for name in ['mT0', 'mT0-MT']])]\n",
        "        for name in EVAL_MODELS:\n",
        "            if scores[name] in best_scores:\n",
        "              print(\" & \\\\textbf{\" + str(round(scores[name] * 100,2)) + \"}\", end = '')\n",
        "            else:\n",
        "              print(\" & \" + str(round(scores[name] * 100,2)), end = '')\n",
        "        print(\" \\\\\\\\\")\n",
        "\n",
        "    print(\"\\\\midrule\")\n",
        "    print(EVAL_TASKS[task_name], end='')\n",
        "    print(\" & EN\", end='')\n",
        "    print_line(score_en)\n",
        "    \n",
        "    print(\" & MT\", end='')\n",
        "    print_line(score_mt)\n",
        "\n",
        "    if task_name == \"xnli\":\n",
        "      print(\" & HT\", end='')\n",
        "      print_line(score_ht)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MkKyxfqcFsd",
        "outputId": "53e75f12-816c-4d66-dbbd-6cc2ecedf548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\midrule\n",
            "XNLI & EN & \\textbf{53.58} & 49.74 & 48.43 & \\textbf{51.52} \\\\\n",
            " & MT & 37.87 & \\textbf{42.03} & 39.83 & \\textbf{42.64} \\\\\n",
            " & HT & 41.13 & \\textbf{44.55} & 45.19 & \\textbf{47.03} \\\\\n",
            "\\midrule\n",
            "XCOPA & EN & 75.5 & \\textbf{75.75} & \\textbf{84.45} & 81.6 \\\\\n",
            " & MT & 71.95 & \\textbf{74.25} & \\textbf{82.9} & 80.2 \\\\\n",
            "\\midrule\n",
            "XStoryCloze & EN & \\textbf{84.42} & 84.07 & 82.52 & \\textbf{82.58} \\\\\n",
            " & MT & 84.37 & \\textbf{85.31} & \\textbf{84.01} & 83.31 \\\\\n",
            "\\midrule\n",
            "XWinograd & EN & \\textbf{60.07} & 59.15 & 70.49 & \\textbf{73.24} \\\\\n",
            " & MT & 58.48 & \\textbf{60.14} & 66.89 & \\textbf{72.33} \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired by Appendix H from GPT-3\n",
        "# Columns: Model ()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Languages not pretrained on\n",
        "l2_xwino = [\"jp\", \"ru\"]\n",
        "l2_xstory = [\"ru\", \"my\"]\n",
        "l2_xcopa = [\"et\", \"ht\", \"it\", \"qu\", \"tr\"]\n",
        "l2_xnli = [\"bg\", \"de\", \"el\", \"ru\", \"th\", \"tr\"]\n",
        "L2 = set(l2_xwino + l2_xstory + l2_xcopa + l2_xnli)\n",
        "\n",
        "# Langs ordered by PCTG in xP3\n",
        "LANGS = ['en', 'es', 'pt', 'fr', 'ar', 'id', 'zh', 'hi', 'code', 'vi', 'ur', 'te', 'ta', 'bn', 'mr', 'sw', 'gu', 'pa', 'ne', 'yo', 'ig', 'ny', 'zu', 'xh', 'sn', 'ts', 'rw', 'lg', 'tn', 'nso', 'rn', 'ml', 'kn', 'or', 'as', 'ln', 'wo', 'tum', 'ki', 'st', 'fon', 'ca', 'eu', 'ak', 'bm', 'tw']\n",
        "EVAL_LANGS = ['en', 'es', 'pt', 'fr', 'ar', 'id', 'zh', 'hi', 'vi', 'ur', 'ta', 'eu',]\n",
        "EVAL_LANGS += [\"Avg\"] # Add Average score at the end\n",
        "\n",
        "DS_TO_NAME = {\n",
        "    \"anli\": \"ANLI\",\n",
        "    \"xnli\": \"XNLI\",\n",
        "    \"super_glue\": \"SuperGLUE\",\n",
        "    \"xcopa\": \"XCOPA\",\n",
        "    \"story_cloze\": \"StoryCloze\",\n",
        "    \"Muennighoff/xstory_cloze\": \"XStoryCloze\",\n",
        "    \"winogrande\": \"Winogrande XL\",\n",
        "    \"Muennighoff/xwinograd\": \"XWinograd\",\n",
        "}\n",
        "\n",
        "DS_TO_SPLIT = {\n",
        "    \"super_glue\": \"validation\",\n",
        "    \"xnli\": \"validation\",\n",
        "    \"story_cloze\": \"validation\",\n",
        "    \"Muennighoff/xstory_cloze\": \"validation\",\n",
        "    \"xcopa\": \"validation\",\n",
        "    \"winogrande\": \"validation\",\n",
        "    \"Muennighoff/xwinograd\": \"test\"\n",
        "}\n",
        "\n",
        "MT5_MODEL_TO_RES = {\n",
        "    \"mT5 XXL\": mt5_xxl\n",
        "}\n",
        "\n",
        "MT0_MODEL_TO_RES = {\n",
        "    \"mT0-13B\": mt0_xxl\n",
        "}\n",
        "\n",
        "BLOOM_MODEL_TO_RES = {\n",
        "    \"BLOOM-560M\": bloom_560m,\n",
        "    \"BLOOM-1.1B\": bloom_1b1,\n",
        "    \"BLOOM-1.7B\": bloom_1b7,\n",
        "    \"BLOOM-3B\": bloom_3b,\n",
        "    \"BLOOM-7.1B\": bloom_7b1,\n",
        "    \"BLOOM\": bloom,\n",
        "}\n",
        "\n",
        "BLOOMZ_MODEL_TO_RES = {\n",
        "    \"BLOOMZ-560M\": bloomz_560m,\n",
        "    \"BLOOMZ-1.1B\": bloomz_1b1,\n",
        "    \"BLOOMZ-1.7B\": bloomz_1b7,\n",
        "    \"BLOOMZ-3B\": bloomz_3b,\n",
        "    \"BLOOMZ-7.1B\": bloomz_7b1,\n",
        "    \"BLOOMZ-7.1B-MT\": bloomz_7b1_mt,\n",
        "    \"BLOOMZ-7.1B-P3\": bloomz_7b1_p3,\n",
        "    \"BLOOMZ\": bloomz,\n",
        "    \"BLOOMZ-MT\": bloomz_mt,\n",
        "    \"BLOOMZ-P3\": bloomz_p3,    \n",
        "}\n",
        "\n",
        "def get_task(task_name):\n",
        "    if \"wino\" in task_name: return \"Coref. Res.\"\n",
        "    elif (\"story\" in task_name) or (\"copa\" in task_name): return \"Completion\"\n",
        "    return \"NLI\"\n",
        "\n",
        "HEADER = \"\\multicolumn{6}{c}{} & \\multicolumn{6}{c}{Pretrained} & \\multicolumn{11}{c}{Pretrained + Multitask finetuned} \\\\\\\\\"\n",
        "HEADER += \"\\n\" + \"Task & Dataset & Config & Split & Prompt & Metric\"\n",
        "HEADER += \" & \" + \" & \".join(list(BLOOM_MODEL_TO_RES.keys()))\n",
        "HEADER += \" & \" + \" & \".join(list(BLOOMZ_MODEL_TO_RES.keys()))\n",
        "HEADER += \" & \" + \" & \".join(list(MT0_MODEL_TO_RES.keys()))\n",
        "HEADER += \" \\\\\\\\\"\n",
        "\n",
        "TABLE = HEADER\n",
        "\n",
        "RES_DICT = {}\n",
        "for ds, ds_name in DS_TO_NAME.items():\n",
        "    print(f\"Running dataset {ds}\")\n",
        "    for name, res_data in {**BLOOM_MODEL_TO_RES, **BLOOMZ_MODEL_TO_RES, **MT0_MODEL_TO_RES}.items():\n",
        "        ds_data = res_data[\"test\"].filter(lambda x: (x[\"evaluation_framework\"]  == \"bigscience/bloomz\") and (x[\"task_name\"].startswith(ds)), load_from_cache_file=False)\n",
        "\n",
        "        # Iterate through subdatasets\n",
        "        for task_name in set(ds_data[\"task_name\"]):\n",
        "            task_ds = ds_data.filter(lambda x: x[\"task_name\"] == task_name)\n",
        "            config = task_name.split(\"_\")[-1]\n",
        "            task = get_task(task_name)\n",
        "            split = DS_TO_SPLIT.get(ds, \"validation\")\n",
        "\n",
        "            #print(res_data)\n",
        "            #print(f\"Unexpected len {set(task_ds['prompt_name'])} for {ds} for {task_name} for {name}.\")\n",
        "\n",
        "            prompt_ds_ht = task_ds.filter(lambda x: x[\"prompt_name\"].endswith(\"ht\"))\n",
        "            prompt_ds_mt = task_ds.filter(lambda x: x[\"prompt_name\"].endswith(\"mt\"))\n",
        "            prompt_ds_en = task_ds.filter(lambda x: not(x[\"prompt_name\"].endswith((\"ht\", \"mt\"))))\n",
        "\n",
        "            for prompt_ds, prompt in [(prompt_ds_en, \"EN\"), (prompt_ds_ht, \"HT\"), (prompt_ds_mt, \"MT\")]:\n",
        "                if len(prompt_ds) == 0: continue\n",
        "                elif len(prompt_ds) != 5: print(f\"Unexpected len {len(prompt_ds)} for {ds} for {task_name} for {prompt} for {name}.\")\n",
        "                score_median = np.median([x[\"score\"] for x in prompt_ds])\n",
        "                score_max = np.max([x[\"score\"] for x in prompt_ds])\n",
        "\n",
        "                RES_DICT.setdefault(task, {})\n",
        "                RES_DICT[task].setdefault(ds, {})\n",
        "                RES_DICT[task][ds].setdefault(config, {})\n",
        "                RES_DICT[task][ds][config].setdefault(split, {})\n",
        "                RES_DICT[task][ds][config][split].setdefault(prompt, {})\n",
        "                RES_DICT[task][ds][config][split][prompt].setdefault(\"Median Acc\", {})\n",
        "                RES_DICT[task][ds][config][split][prompt].setdefault(\"Max Acc\", {})\n",
        "                RES_DICT[task][ds][config][split][prompt][\"Median Acc\"][name] = format(score_median * 100, '.2f')\n",
        "                RES_DICT[task][ds][config][split][prompt][\"Max Acc\"][name] = format(score_max * 100, '.2f')\n",
        "\n",
        "\n",
        "task, ds, config, split, prompt = \"Program Synthesis\", \"openai_humaneval\", \"None\", \"test\", \"EN\"\n",
        "for name, res_data in {**BLOOM_MODEL_TO_RES, **BLOOMZ_MODEL_TO_RES, **MT0_MODEL_TO_RES}.items():\n",
        "    ds_data = res_data[\"test\"].filter(lambda x: (x[\"evaluation_framework\"]  == \"bloom-code-evaluation\"), load_from_cache_file=False)\n",
        "    for k in [1, 10, 100]:\n",
        "        k_data = ds_data.filter(lambda x: x[\"metric\"].startswith(f\"pass@{k}-\"))\n",
        "        if len(k_data) == 0: continue\n",
        "        RES_DICT.setdefault(task, {})\n",
        "        RES_DICT[task].setdefault(ds, {})\n",
        "        RES_DICT[task][ds].setdefault(config, {})\n",
        "        RES_DICT[task][ds][config].setdefault(split, {})\n",
        "        RES_DICT[task][ds][config][split].setdefault(prompt, {})\n",
        "        RES_DICT[task][ds][config][split][prompt].setdefault(f\"Pass@{k}\", {})\n",
        "        RES_DICT[task][ds][config][split][prompt][f\"Pass@{k}\"][name] = format(np.max(k_data[\"score\"]) * 100, '.2f')\n",
        "\n",
        "\n",
        "for task, vals in RES_DICT.items():\n",
        "    for ds, sub_vals in sorted(vals.items()):\n",
        "        for config, sub_sub_vals in sorted(sub_vals.items()):\n",
        "            for split, sub_sub_sub_vals in sorted(sub_sub_vals.items()):\n",
        "                for prompt, sub_sub_sub_sub_vals in sorted(sub_sub_sub_vals.items()):\n",
        "                    for metric, sub_sub_sub_sub_sub_vals in sorted(sub_sub_sub_sub_vals.items()):\n",
        "                        ONE_LINE = f\"{task} & {ds} & {config} & {split} & {prompt} & {metric}\"\n",
        "                        #for name in MT5_MODEL_TO_RES:\n",
        "                        #    ONE_LINE += \" & \" + sub_sub_sub_sub_sub_vals.get(name, \"-\")\n",
        "                        for name in BLOOM_MODEL_TO_RES:\n",
        "                            ONE_LINE += \" & \" + sub_sub_sub_sub_sub_vals.get(name, \"-\")\n",
        "                        for name in BLOOMZ_MODEL_TO_RES:\n",
        "                            ONE_LINE += \" & \" + sub_sub_sub_sub_sub_vals.get(name, \"-\")\n",
        "                        for name in MT0_MODEL_TO_RES:\n",
        "                            ONE_LINE += \" & \" + sub_sub_sub_sub_sub_vals.get(name, \"-\")                            \n",
        "                        TABLE += \"\\n\" + ONE_LINE + \"\\\\\\\\\"\n",
        "\n",
        "# Escape _ in Latex\n",
        "TABLE = TABLE.replace(\"_\", \"\\_\")\n",
        "TABLE = TABLE.replace(\"Muennighoff/\", \"\")\n"
      ],
      "metadata": {
        "id": "pkx4dnV1ocR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TABLE)"
      ],
      "metadata": {
        "id": "QoJL3nyDNcaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z6u8IqWcu4bd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}